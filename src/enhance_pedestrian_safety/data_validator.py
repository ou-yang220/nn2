import json
import os
import numpy as np
from datetime import datetime


class DataValidator:
    """æ•°æ®éªŒè¯å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    @staticmethod
    def validate_dataset(data_dir, detailed=False):
        """éªŒè¯æ•°æ®é›†ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print(f"\n{'=' * 60}")
        print(f"éªŒè¯æ•°æ®é›†: {data_dir}")
        print(f"{'=' * 60}")

        validation_results = {
            'validation_time': datetime.now().isoformat(),
            'dataset_path': data_dir,
            'directory_structure': DataValidator._check_directory_structure(data_dir),
            'raw_images': DataValidator._validate_raw_images(data_dir),
            'stitched_images': DataValidator._validate_stitched_images(data_dir),
            'annotations': DataValidator._validate_annotations(data_dir),
            'metadata': DataValidator._validate_metadata(data_dir),
            'lidar_data': DataValidator._validate_lidar_data(data_dir),
            'cooperative_data': DataValidator._validate_cooperative_data(data_dir),
            'fusion_data': DataValidator._validate_fusion_data(data_dir),
            'safety_data': DataValidator._validate_safety_data(data_dir),
            'enhanced_data': DataValidator._validate_enhanced_data(data_dir),
            'calibration_data': DataValidator._validate_calibration_data(data_dir),
            'timestamps': DataValidator._validate_timestamps(data_dir)
        }

        # è®¡ç®—å®‰å…¨æŒ‡æ ‡
        validation_results['safety_metrics'] = DataValidator._calculate_safety_metrics(data_dir)

        validation_results['overall_score'] = DataValidator._calculate_score(validation_results)
        validation_results['health_status'] = DataValidator._get_health_status(validation_results['overall_score'])

        # è¯¦ç»†éªŒè¯
        if detailed:
            validation_results['detailed_analysis'] = DataValidator._detailed_analysis(data_dir)

        DataValidator._save_validation_report(data_dir, validation_results)
        DataValidator._print_validation_report(validation_results)

        return validation_results

    @staticmethod
    def _check_directory_structure(data_dir):
        """æ£€æŸ¥ç›®å½•ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        required_dirs = [
            "raw/vehicle_1",
            "raw/infrastructure",
            "stitched",
            "metadata",
            "cooperative/v2x_messages",
            "cooperative/shared_perception",
            "fusion",
            "annotations"
        ]

        optional_dirs = [
            "lidar",
            "calibration",
            "safety_reports",
            "enhanced",
            "v2xformer_format",
            "kitti_format",
            "risk_maps",
            "emergency_events"
        ]

        missing_dirs = []
        for dir_path in required_dirs:
            full_path = os.path.join(data_dir, dir_path)
            if not os.path.exists(full_path):
                missing_dirs.append(dir_path)

        missing_optional = []
        for dir_path in optional_dirs:
            full_path = os.path.join(data_dir, dir_path)
            if not os.path.exists(full_path):
                missing_optional.append(dir_path)

        # æ£€æŸ¥æ–‡ä»¶æ•°é‡
        dir_stats = {}
        total_files = 0
        for root, dirs, files in os.walk(data_dir):
            if root.startswith(os.path.join(data_dir, '.')):
                continue  # è·³è¿‡éšè—ç›®å½•

            rel_path = os.path.relpath(root, data_dir)
            if rel_path == '.':
                rel_path = 'root'

            dir_stats[rel_path] = len(files)
            total_files += len(files)

        status = 'PASS' if len(missing_dirs) == 0 else 'FAIL'

        result = {
            'status': status,
            'missing_directories': missing_dirs,
            'missing_optional_directories': missing_optional,
            'directory_stats': dir_stats,
            'total_files': total_files,
            'required_directories': required_dirs,
            'optional_directories': optional_dirs
        }

        return result

    @staticmethod
    def _validate_raw_images(data_dir):
        """éªŒè¯åŸå§‹å›¾åƒï¼ˆå¢å¼ºç‰ˆï¼‰"""
        raw_path = os.path.join(data_dir, "raw")

        if not os.path.exists(raw_path):
            return {'vehicle': {'status': 'MISSING', 'count': 0, 'errors': [], 'sizes': []},
                    'infrastructure': {'status': 'MISSING', 'count': 0, 'errors': [], 'sizes': []}}

        raw_dirs = [d for d in os.listdir(raw_path) if os.path.isdir(os.path.join(raw_path, d))]
        results = {}

        for raw_dir in raw_dirs:
            path = os.path.join(data_dir, "raw", raw_dir)

            camera_dirs = []
            if os.path.exists(path):
                camera_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]

            total_images = 0
            errors = []
            image_sizes = []

            for camera_dir in camera_dirs:
                camera_path = os.path.join(path, camera_dir)
                images = [f for f in os.listdir(camera_path) if f.endswith(('.png', '.jpg', '.jpeg'))]

                for img_file in images:
                    img_path = os.path.join(camera_path, img_file)
                    try:
                        file_size = os.path.getsize(img_path)
                        if file_size == 0:
                            errors.append(f"ç©ºæ–‡ä»¶: {img_file}")
                        else:
                            image_sizes.append(file_size)
                    except:
                        errors.append(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {img_file}")

                total_images += len(images)

            if len(errors) == 0 and total_images > 0:
                status = 'PASS'
            elif len(errors) < 5 and total_images > 0:
                status = 'WARNING'
            else:
                status = 'FAIL'

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = {}
            if image_sizes:
                stats = {
                    'min_size_kb': min(image_sizes) / 1024,
                    'max_size_kb': max(image_sizes) / 1024,
                    'avg_size_kb': np.mean(image_sizes) / 1024,
                    'total_size_mb': sum(image_sizes) / (1024 * 1024)
                }

            results[raw_dir] = {
                'status': status,
                'count': total_images,
                'camera_count': len(camera_dirs),
                'errors': errors,
                'statistics': stats
            }

        return results

    @staticmethod
    def _validate_stitched_images(data_dir):
        """éªŒè¯æ‹¼æ¥å›¾åƒï¼ˆå¢å¼ºç‰ˆï¼‰"""
        stitched_dir = os.path.join(data_dir, "stitched")

        if not os.path.exists(stitched_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'statistics': {}}

        images = [f for f in os.listdir(stitched_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
        errors = []
        image_sizes = []

        for img_file in images:
            img_path = os.path.join(stitched_dir, img_file)
            try:
                file_size = os.path.getsize(img_path)
                if file_size == 0:
                    errors.append(f"ç©ºæ–‡ä»¶: {img_file}")
                else:
                    image_sizes.append(file_size)
            except:
                errors.append(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {img_file}")

        if len(errors) == 0 and len(images) > 0:
            status = 'PASS'
        elif len(errors) < 3 and len(images) > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        if image_sizes:
            stats = {
                'min_size_kb': min(image_sizes) / 1024,
                'max_size_kb': max(image_sizes) / 1024,
                'avg_size_kb': np.mean(image_sizes) / 1024,
                'total_size_mb': sum(image_sizes) / (1024 * 1024)
            }

        return {
            'status': status,
            'count': len(images),
            'errors': errors,
            'statistics': stats
        }

    @staticmethod
    def _validate_annotations(data_dir):
        """éªŒè¯æ ‡æ³¨æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        annotations_dir = os.path.join(data_dir, "annotations")

        if not os.path.exists(annotations_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'frame_range': []}

        json_files = [f for f in os.listdir(annotations_dir) if f.endswith('.json')]
        errors = []
        valid_files = 0
        frame_ids = []

        for json_file in json_files:
            json_path = os.path.join(annotations_dir, json_file)
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                # æ£€æŸ¥åŸºæœ¬ç»“æ„
                required_keys = ['frame_id', 'objects', 'safety_info']
                for key in required_keys:
                    if key not in data:
                        errors.append(f"ç¼ºå¤±å¿…è¦é”®: {key} in {json_file}")

                # æ£€æŸ¥å®‰å…¨ä¿¡æ¯
                if 'safety_info' in data:
                    safety_info = data['safety_info']
                    if 'pedestrian_count' not in safety_info or 'vehicle_count' not in safety_info:
                        errors.append(f"ç¼ºå¤±å®‰å…¨ç»Ÿè®¡ä¿¡æ¯: {json_file}")

                # è®°å½•å¸§ID
                if 'frame_id' in data:
                    frame_ids.append(data['frame_id'])

                valid_files += 1
            except Exception as e:
                errors.append(f"æ— æ•ˆçš„JSONæ–‡ä»¶: {json_file} - {str(e)}")

        if len(errors) == 0 and valid_files > 0:
            status = 'PASS'
        elif len(errors) < 5 and valid_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        # è®¡ç®—å¸§èŒƒå›´
        frame_range = []
        if frame_ids:
            frame_range = [min(frame_ids), max(frame_ids)]

        return {
            'status': status,
            'count': valid_files,
            'total_files': len(json_files),
            'frame_range': frame_range,
            'errors': errors
        }

    @staticmethod
    def _validate_metadata(data_dir):
        """éªŒè¯å…ƒæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        metadata_dir = os.path.join(data_dir, "metadata")

        if not os.path.exists(metadata_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'files': []}

        json_files = [f for f in os.listdir(metadata_dir) if f.endswith('.json')]
        errors = []
        valid_files = 0
        file_details = []

        required_files = ['collection_info.json', 'scene_description.json']
        missing_files = []

        for req_file in required_files:
            if req_file not in json_files:
                missing_files.append(req_file)

        for json_file in json_files:
            json_path = os.path.join(metadata_dir, json_file)
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(json_path)

                file_details.append({
                    'file': json_file,
                    'size_kb': file_size / 1024,
                    'keys': list(data.keys())
                })

                valid_files += 1
            except Exception as e:
                errors.append(f"æ— æ•ˆçš„JSONæ–‡ä»¶: {json_file} - {str(e)}")

        if len(errors) == 0 and valid_files > 0:
            status = 'PASS'
        elif len(errors) < 3 and valid_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        if missing_files:
            errors.extend([f"ç¼ºå¤±å¿…è¦å…ƒæ•°æ®æ–‡ä»¶: {f}" for f in missing_files])

        return {
            'status': status,
            'count': valid_files,
            'files': file_details,
            'missing_files': missing_files,
            'errors': errors
        }

    @staticmethod
    def _validate_lidar_data(data_dir):
        """éªŒè¯LiDARæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        lidar_dir = os.path.join(data_dir, "lidar")

        if not os.path.exists(lidar_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'statistics': {}}

        bin_files = [f for f in os.listdir(lidar_dir) if f.endswith('.bin')]
        npy_files = [f for f in os.listdir(lidar_dir) if f.endswith('.npy')]
        json_files = [f for f in os.listdir(lidar_dir) if f.endswith('.json')]

        errors = []
        valid_bin_files = 0
        file_sizes = []

        for bin_file in bin_files:
            bin_path = os.path.join(lidar_dir, bin_file)
            try:
                file_size = os.path.getsize(bin_path)
                if file_size > 0:
                    valid_bin_files += 1
                    file_sizes.append(file_size)
                else:
                    errors.append(f"ç©ºæ–‡ä»¶: {bin_file}")
            except:
                errors.append(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {bin_file}")

        total_files = len(bin_files) + len(npy_files) + len(json_files)

        if len(errors) == 0 and valid_bin_files > 0:
            status = 'PASS'
        elif len(errors) < 5 and valid_bin_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {}
        if file_sizes:
            stats = {
                'min_size_mb': min(file_sizes) / (1024 * 1024),
                'max_size_mb': max(file_sizes) / (1024 * 1024),
                'avg_size_mb': np.mean(file_sizes) / (1024 * 1024),
                'total_size_gb': sum(file_sizes) / (1024 * 1024 * 1024)
            }

        return {
            'status': status,
            'count': total_files,
            'bin_files': len(bin_files),
            'npy_files': len(npy_files),
            'json_files': len(json_files),
            'valid_bin_files': valid_bin_files,
            'errors': errors,
            'statistics': stats
        }

    @staticmethod
    def _validate_cooperative_data(data_dir):
        """éªŒè¯ååŒæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        coop_dir = os.path.join(data_dir, "cooperative")

        if not os.path.exists(coop_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'v2x_messages': 0, 'shared_perception': 0}

        errors = []

        # æ£€æŸ¥V2Xæ¶ˆæ¯ç›®å½•
        v2x_dir = os.path.join(coop_dir, "v2x_messages")
        v2x_files = []
        v2x_stats = {'total': 0, 'types': {}}

        if os.path.exists(v2x_dir):
            v2x_files = [f for f in os.listdir(v2x_dir) if f.endswith('.json')]
            v2x_stats['total'] = len(v2x_files)

            for v2x_file in v2x_files[:min(10, len(v2x_files))]:
                try:
                    with open(os.path.join(v2x_dir, v2x_file), 'r') as f:
                        data = json.load(f)

                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    required_keys = ['message', 'recipients', 'transmission_time']
                    for key in required_keys:
                        if key not in data:
                            errors.append(f"V2Xæ¶ˆæ¯ç¼ºå¤±å­—æ®µ {key}: {v2x_file}")

                    # ç»Ÿè®¡æ¶ˆæ¯ç±»å‹
                    if 'message' in data and 'message_type' in data['message']:
                        msg_type = data['message']['message_type']
                        v2x_stats['types'][msg_type] = v2x_stats['types'].get(msg_type, 0) + 1

                except Exception as e:
                    errors.append(f"V2Xæ¶ˆæ¯æ–‡ä»¶æ— æ•ˆ: {v2x_file} - {str(e)}")
        else:
            errors.append("V2Xæ¶ˆæ¯ç›®å½•ä¸å­˜åœ¨")

        # æ£€æŸ¥å…±äº«æ„ŸçŸ¥ç›®å½•
        perception_dir = os.path.join(coop_dir, "shared_perception")
        perception_files = []
        perception_stats = {'total': 0, 'avg_objects': 0}

        if os.path.exists(perception_dir):
            perception_files = [f for f in os.listdir(perception_dir) if f.endswith('.json')]
            perception_stats['total'] = len(perception_files)

            object_counts = []
            for perception_file in perception_files[:min(10, len(perception_files))]:
                try:
                    with open(os.path.join(perception_dir, perception_file), 'r') as f:
                        data = json.load(f)

                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    required_keys = ['frame_id', 'timestamp', 'shared_objects']
                    for key in required_keys:
                        if key not in data:
                            errors.append(f"å…±äº«æ„ŸçŸ¥æ–‡ä»¶ç¼ºå¤±å­—æ®µ {key}: {perception_file}")

                    # ç»Ÿè®¡å¯¹è±¡æ•°é‡
                    if 'shared_objects' in data:
                        object_counts.append(len(data['shared_objects']))

                except Exception as e:
                    errors.append(f"å…±äº«æ„ŸçŸ¥æ–‡ä»¶æ— æ•ˆ: {perception_file} - {str(e)}")

            if object_counts:
                perception_stats['avg_objects'] = np.mean(object_counts)
        else:
            errors.append("å…±äº«æ„ŸçŸ¥ç›®å½•ä¸å­˜åœ¨")

        # æ£€æŸ¥ååŒæ‘˜è¦
        summary_file = os.path.join(coop_dir, "cooperative_summary.json")
        summary_valid = False
        if not os.path.exists(summary_file):
            errors.append("ååŒæ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨")
        else:
            try:
                with open(summary_file, 'r') as f:
                    data = json.load(f)

                # æ£€æŸ¥æ‘˜è¦å­—æ®µ
                required_keys = ['total_vehicles', 'ego_vehicles', 'cooperative_vehicles', 'v2x_stats']
                for key in required_keys:
                    if key not in data:
                        errors.append(f"ååŒæ‘˜è¦ç¼ºå¤±å­—æ®µ: {key}")

                summary_valid = True
            except Exception as e:
                errors.append(f"ååŒæ‘˜è¦æ–‡ä»¶æ— æ•ˆ: {str(e)}")

        total_files = len(v2x_files) + len(perception_files)

        if len(errors) == 0 and total_files > 0 and summary_valid:
            status = 'PASS'
        elif len(errors) < 5 and total_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        return {
            'status': status,
            'count': total_files,
            'v2x_messages': v2x_stats,
            'shared_perception': perception_stats,
            'summary_valid': summary_valid,
            'errors': errors
        }

    @staticmethod
    def _validate_fusion_data(data_dir):
        """éªŒè¯èåˆæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        fusion_dir = os.path.join(data_dir, "fusion")

        if not os.path.exists(fusion_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'statistics': {}}

        json_files = [f for f in os.listdir(fusion_dir) if f.endswith('.json')]
        gz_files = [f for f in os.listdir(fusion_dir) if f.endswith('.gz')]
        all_files = json_files + gz_files

        errors = []
        valid_files = 0
        sensor_types = set()

        for json_file in all_files[:min(10, len(all_files))]:
            json_path = os.path.join(fusion_dir, json_file)
            try:
                # å¤„ç†å‹ç¼©æ–‡ä»¶
                if json_file.endswith('.gz'):
                    import gzip
                    with gzip.open(json_path, 'rt', encoding='utf-8') as f:
                        data = json.load(f)
                else:
                    with open(json_path, 'r') as f:
                        data = json.load(f)

                # æ£€æŸ¥å¿…è¦å­—æ®µ
                required_keys = ['frame_id', 'timestamp', 'sensors']
                for key in required_keys:
                    if key not in data:
                        errors.append(f"èåˆæ–‡ä»¶ç¼ºå¤±å­—æ®µ {key}: {json_file}")

                # æ”¶é›†ä¼ æ„Ÿå™¨ç±»å‹
                if 'sensors' in data:
                    sensor_types.update(data['sensors'].keys())

                valid_files += 1
            except Exception as e:
                errors.append(f"èåˆæ–‡ä»¶æ— æ•ˆ: {json_file} - {str(e)}")

        if len(errors) == 0 and valid_files > 0:
            status = 'PASS'
        elif len(errors) < 5 and valid_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        return {
            'status': status,
            'count': len(all_files),
            'json_files': len(json_files),
            'gz_files': len(gz_files),
            'valid_files': valid_files,
            'sensor_types': list(sensor_types),
            'errors': errors
        }

    @staticmethod
    def _validate_safety_data(data_dir):
        """éªŒè¯å®‰å…¨æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        safety_dir = os.path.join(data_dir, "safety_reports")

        if not os.path.exists(safety_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': [], 'risk_stats': {}}

        json_files = [f for f in os.listdir(safety_dir) if f.endswith('.json')]
        errors = []
        valid_files = 0
        risk_stats = {'high': 0, 'medium': 0, 'low': 0, 'critical': 0}

        for json_file in json_files[:min(10, len(json_files))]:
            json_path = os.path.join(safety_dir, json_file)
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                # æ£€æŸ¥å¿…è¦å­—æ®µ
                required_keys = ['timestamp', 'total_interactions']
                for key in required_keys:
                    if key not in data:
                        errors.append(f"å®‰å…¨æŠ¥å‘Šç¼ºå¤±å­—æ®µ {key}: {json_file}")

                # ç»Ÿè®¡é£é™©çº§åˆ«
                if 'risk_level' in data:
                    risk_level = data['risk_level']
                    if risk_level in risk_stats:
                        risk_stats[risk_level] += 1

                valid_files += 1
            except Exception as e:
                errors.append(f"å®‰å…¨æŠ¥å‘Šæ— æ•ˆ: {json_file} - {str(e)}")

        if len(errors) == 0 and valid_files > 0:
            status = 'PASS'
        elif len(errors) < 5 and valid_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        return {
            'status': status,
            'count': len(json_files),
            'valid_files': valid_files,
            'risk_stats': risk_stats,
            'errors': errors
        }

    @staticmethod
    def _validate_enhanced_data(data_dir):
        """éªŒè¯å¢å¼ºæ•°æ®"""
        enhanced_dir = os.path.join(data_dir, "enhanced")

        if not os.path.exists(enhanced_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': []}

        # æŸ¥æ‰¾æ‰€æœ‰å¢å¼ºå›¾åƒ
        enhanced_files = []
        for root, dirs, files in os.walk(enhanced_dir):
            for file in files:
                if file.endswith(('.png', '.jpg', '.jpeg')):
                    enhanced_files.append(os.path.join(root, file))

        errors = []

        # æ£€æŸ¥æ ·æœ¬æ–‡ä»¶
        sample_files = enhanced_files[:min(5, len(enhanced_files))]
        for file_path in sample_files:
            try:
                if os.path.getsize(file_path) == 0:
                    errors.append(f"ç©ºå¢å¼ºæ–‡ä»¶: {os.path.basename(file_path)}")
            except:
                errors.append(f"æ–‡ä»¶è®¿é—®å¤±è´¥: {os.path.basename(file_path)}")

        if len(errors) == 0 and len(enhanced_files) > 0:
            status = 'PASS'
        elif len(errors) < 3 and len(enhanced_files) > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        return {
            'status': status,
            'count': len(enhanced_files),
            'errors': errors
        }

    @staticmethod
    def _validate_calibration_data(data_dir):
        """éªŒè¯æ ‡å®šæ•°æ®"""
        calib_dir = os.path.join(data_dir, "calibration")

        if not os.path.exists(calib_dir):
            return {'status': 'MISSING', 'count': 0, 'errors': []}

        json_files = [f for f in os.listdir(calib_dir) if f.endswith('.json')]
        errors = []
        valid_files = 0

        for json_file in json_files:
            json_path = os.path.join(calib_dir, json_file)
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)
                valid_files += 1
            except Exception as e:
                errors.append(f"æ ‡å®šæ–‡ä»¶æ— æ•ˆ: {json_file} - {str(e)}")

        if len(errors) == 0 and valid_files > 0:
            status = 'PASS'
        elif len(errors) < 3 and valid_files > 0:
            status = 'WARNING'
        else:
            status = 'FAIL'

        return {
            'status': status,
            'count': valid_files,
            'errors': errors
        }

    @staticmethod
    def _validate_timestamps(data_dir):
        """éªŒè¯æ—¶é—´æˆ³è¿ç»­æ€§"""
        annotations_dir = os.path.join(data_dir, "annotations")

        if not os.path.exists(annotations_dir):
            return {'status': 'SKIPPED', 'errors': ['æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨']}

        json_files = [f for f in os.listdir(annotations_dir) if f.endswith('.json')]
        if len(json_files) < 2:
            return {'status': 'SKIPPED', 'errors': ['æ ‡æ³¨æ–‡ä»¶ä¸è¶³']}

        timestamps = []
        errors = []

        # æŒ‰æ–‡ä»¶åæ’åº
        json_files.sort()

        for json_file in json_files[:min(100, len(json_files))]:
            json_path = os.path.join(annotations_dir, json_file)
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                if 'timestamp' in data:
                    timestamps.append(data['timestamp'])
                elif 'frame_id' in data:
                    timestamps.append(data['frame_id'])
            except:
                continue

        if len(timestamps) < 2:
            return {'status': 'SKIPPED', 'errors': ['æœ‰æ•ˆæ—¶é—´æˆ³ä¸è¶³']}

        # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦é€’å¢
        for i in range(1, len(timestamps)):
            if isinstance(timestamps[i], (int, float)) and isinstance(timestamps[i - 1], (int, float)):
                if timestamps[i] <= timestamps[i - 1]:
                    errors.append(f"æ—¶é—´æˆ³ä¸é€’å¢: {timestamps[i - 1]} -> {timestamps[i]}")

        if len(errors) == 0:
            status = 'PASS'
        elif len(errors) < 3:
            status = 'WARNING'
        else:
            status = 'FAIL'

        return {
            'status': status,
            'timestamp_count': len(timestamps),
            'errors': errors
        }

    @staticmethod
    def _calculate_safety_metrics(data_dir):
        """è®¡ç®—å®‰å…¨æŒ‡æ ‡"""
        safety_dir = os.path.join(data_dir, "safety_reports")

        if not os.path.exists(safety_dir):
            return {'status': 'MISSING', 'metrics': {}}

        json_files = [f for f in os.listdir(safety_dir) if f.endswith('.json')]

        total_high_risk = 0
        total_interactions = 0
        safety_scores = []

        for json_file in json_files[:min(20, len(json_files))]:
            json_path = os.path.join(safety_dir, json_file)
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)

                if 'high_risk_cases' in data:
                    total_high_risk += data['high_risk_cases']
                if 'total_interactions' in data:
                    total_interactions += data['total_interactions']
                if 'safety_score' in data:
                    safety_scores.append(data['safety_score'])
            except:
                continue

        metrics = {}
        if total_interactions > 0:
            metrics['high_risk_ratio'] = total_high_risk / total_interactions
            metrics['interaction_density'] = total_interactions / len(json_files) if json_files else 0

        if safety_scores:
            metrics['avg_safety_score'] = np.mean(safety_scores)
            metrics['min_safety_score'] = min(safety_scores)
            metrics['max_safety_score'] = max(safety_scores)

        return {'status': 'CALCULATED', 'metrics': metrics}

    @staticmethod
    def _calculate_score(results):
        """è®¡ç®—æ€»ä½“è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        weights = {
            'directory_structure': 0.10,
            'raw_images': 0.15,
            'stitched_images': 0.05,
            'annotations': 0.10,
            'metadata': 0.08,
            'lidar_data': 0.10,
            'cooperative_data': 0.10,
            'fusion_data': 0.08,
            'safety_data': 0.12,
            'enhanced_data': 0.04,
            'calibration_data': 0.03,
            'timestamps': 0.03,
            'safety_metrics': 0.02
        }

        score = 0
        for key, weight in weights.items():
            if key not in results:
                score += 30 * weight
                continue

            result = results[key]
            if 'status' not in result:
                score += 30 * weight
                continue

            if result['status'] == 'PASS':
                score += 100 * weight
            elif result['status'] == 'WARNING':
                score += 70 * weight
            elif result['status'] == 'FAIL':
                score += 30 * weight
            elif result['status'] == 'MISSING':
                score += 20 * weight
            elif result['status'] == 'SKIPPED':
                score += 50 * weight
            else:
                score += 50 * weight

        return round(min(100, max(0, score)), 1)

    @staticmethod
    def _get_health_status(score):
        """è·å–å¥åº·çŠ¶æ€"""
        if score >= 90:
            return 'EXCELLENT'
        elif score >= 75:
            return 'GOOD'
        elif score >= 60:
            return 'FAIR'
        elif score >= 40:
            return 'POOR'
        else:
            return 'CRITICAL'

    @staticmethod
    def _detailed_analysis(data_dir):
        """è¯¦ç»†åˆ†æ"""
        analysis = {
            'file_size_distribution': DataValidator._analyze_file_sizes(data_dir),
            'data_consistency': DataValidator._check_data_consistency(data_dir),
            'completeness': DataValidator._check_completeness(data_dir)
        }
        return analysis

    @staticmethod
    def _analyze_file_sizes(data_dir):
        """åˆ†ææ–‡ä»¶å¤§å°åˆ†å¸ƒ"""
        size_ranges = {
            'tiny': 0,  # < 1KB
            'small': 0,  # 1KB - 100KB
            'medium': 0,  # 100KB - 1MB
            'large': 0,  # 1MB - 10MB
            'huge': 0  # > 10MB
        }

        total_size = 0
        file_count = 0

        for root, dirs, files in os.walk(data_dir):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    size = os.path.getsize(file_path)
                    total_size += size
                    file_count += 1

                    if size < 1024:
                        size_ranges['tiny'] += 1
                    elif size < 1024 * 100:
                        size_ranges['small'] += 1
                    elif size < 1024 * 1024:
                        size_ranges['medium'] += 1
                    elif size < 1024 * 1024 * 10:
                        size_ranges['large'] += 1
                    else:
                        size_ranges['huge'] += 1
                except:
                    continue

        return {
            'total_files': file_count,
            'total_size_gb': total_size / (1024 ** 3),
            'avg_file_size_kb': (total_size / max(1, file_count)) / 1024,
            'size_distribution': size_ranges
        }

    @staticmethod
    def _check_data_consistency(data_dir):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶å’Œå›¾åƒæ–‡ä»¶æ•°é‡æ˜¯å¦åŒ¹é…
        annotations_dir = os.path.join(data_dir, "annotations")
        raw_dir = os.path.join(data_dir, "raw")

        annotation_count = 0
        image_count = 0

        if os.path.exists(annotations_dir):
            annotation_count = len([f for f in os.listdir(annotations_dir) if f.endswith('.json')])

        if os.path.exists(raw_dir):
            for root, dirs, files in os.walk(raw_dir):
                for file in files:
                    if file.endswith(('.png', '.jpg', '.jpeg')):
                        image_count += 1

        consistency = abs(annotation_count - image_count) <= max(annotation_count, image_count) * 0.1

        return {
            'annotation_count': annotation_count,
            'image_count': image_count,
            'consistency': consistency,
            'difference': abs(annotation_count - image_count)
        }

    @staticmethod
    def _check_completeness(data_dir):
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        completeness = {}

        # æ£€æŸ¥å¿…è¦ç›®å½•
        required_dirs = ['raw', 'annotations', 'metadata']
        for dir_name in required_dirs:
            dir_path = os.path.join(data_dir, dir_name)
            completeness[dir_name] = os.path.exists(dir_path) and len(os.listdir(dir_path)) > 0

        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        metadata_dir = os.path.join(data_dir, "metadata")
        if os.path.exists(metadata_dir):
            required_files = ['collection_info.json']
            for file_name in required_files:
                file_path = os.path.join(metadata_dir, file_name)
                completeness[file_name] = os.path.exists(file_path)

        return completeness

    @staticmethod
    def _save_validation_report(data_dir, results):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        report_path = os.path.join(data_dir, "metadata", "validation_report.json")

        os.makedirs(os.path.dirname(report_path), exist_ok=True)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\néªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    @staticmethod
    def _print_validation_report(results):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("æ•°æ®é›†éªŒè¯æŠ¥å‘Š")
        print("=" * 60)

        # æ€»ä½“ä¿¡æ¯
        print(f"\nğŸ“Š æ€»ä½“ä¿¡æ¯:")
        print(f"  æ•°æ®é›†è·¯å¾„: {results['dataset_path']}")
        print(f"  éªŒè¯æ—¶é—´: {results['validation_time']}")
        print(f"  æ€»ä½“è¯„åˆ†: {results['overall_score']}/100")
        print(f"  å¥åº·çŠ¶æ€: {results['health_status']}")

        # å„æ¨¡å—éªŒè¯ç»“æœ
        print(f"\nğŸ” è¯¦ç»†éªŒè¯ç»“æœ:")

        for key, result in results.items():
            if key in ['overall_score', 'health_status', 'dataset_path', 'validation_time',
                       'safety_metrics', 'detailed_analysis']:
                continue

            print(f"\n{key.replace('_', ' ').title()}:")

            if 'status' in result:
                status_icon = 'âœ“' if result['status'] == 'PASS' else 'âš ' if result['status'] in ['WARNING',
                                                                                                 'SKIPPED'] else 'âœ—'
                print(f"  çŠ¶æ€: {status_icon} {result['status']}")

            if 'count' in result:
                if isinstance(result['count'], dict):
                    print(f"  ç»Ÿè®¡: {json.dumps(result['count'], indent=2)}")
                else:
                    print(f"  æ•°é‡: {result['count']}")

            # ç‰¹å®šç±»å‹æ•°æ®çš„é¢å¤–ä¿¡æ¯
            if key == 'raw_images' and isinstance(result, dict):
                for subkey, subresult in result.items():
                    if isinstance(subresult, dict) and 'count' in subresult:
                        print(f"    {subkey}: {subresult['count']} å›¾åƒ")
                        if 'statistics' in subresult and subresult['statistics']:
                            stats = subresult['statistics']
                            print(f"      å¤§å°: {stats.get('avg_size_kb', 0):.1f} KB/å¼ ")

            if key == 'safety_data' and isinstance(result, dict):
                if 'risk_stats' in result:
                    print(f"    é£é™©ç»Ÿè®¡: {json.dumps(result['risk_stats'], indent=2)}")

            if 'errors' in result and result['errors']:
                print(f"  é”™è¯¯ ({len(result['errors'])}):")
                for error in result['errors'][:3]:
                    print(f"    - {error}")
                if len(result['errors']) > 3:
                    print(f"    ... è¿˜æœ‰ {len(result['errors']) - 3} ä¸ªé”™è¯¯")

        # å®‰å…¨æŒ‡æ ‡
        if 'safety_metrics' in results and results['safety_metrics']['status'] != 'MISSING':
            print(f"\nğŸ›¡ï¸ å®‰å…¨æŒ‡æ ‡:")
            metrics = results['safety_metrics']['metrics']
            for key, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.3f}")
                else:
                    print(f"  {key}: {value}")

        # å»ºè®®
        print(f"\nğŸ’¡ å»ºè®®:")
        overall_score = results.get('overall_score', 0)
        if overall_score >= 90:
            print("  âœ“ æ•°æ®é›†è´¨é‡ä¼˜ç§€ï¼Œå¯ç›´æ¥ä½¿ç”¨")
        elif overall_score >= 75:
            print("  âœ“ æ•°æ®é›†è´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œå°‘é‡ä¼˜åŒ–")
        elif overall_score >= 60:
            print("  âš  æ•°æ®é›†è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®è¿›è¡Œä¼˜åŒ–")
            if results.get('directory_structure', {}).get('missing_directories'):
                print("    - è¡¥å…¨ç¼ºå¤±çš„å¿…è¦ç›®å½•")
            if results.get('raw_images', {}).get('vehicle', {}).get('count', 0) < 10:
                print("    - å¢åŠ è½¦è¾†å›¾åƒæ•°é‡")
        else:
            print("  âœ— æ•°æ®é›†è´¨é‡éœ€è¦é‡å¤§æ”¹è¿›")
            print("    - æ£€æŸ¥æ•°æ®é‡‡é›†è¿‡ç¨‹")
            print("    - éªŒè¯ä¼ æ„Ÿå™¨é…ç½®")
            print("    - é‡æ–°æ”¶é›†å…³é”®æ•°æ®")

        print("\n" + "=" * 60)